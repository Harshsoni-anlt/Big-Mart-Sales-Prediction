{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "076cda77",
   "metadata": {},
   "source": [
    "# BigMart Sales Prediction - Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a649a9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/V3_env/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab6e2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (8523, 12)\n",
      "Test data: (5681, 11)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv('../data/train.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(f\"Training data: {train_data.shape}\")\n",
    "print(f\"Test data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30bf97d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (14204, 13)\n"
     ]
    }
   ],
   "source": [
    "# Combine datasets for consistent feature engineering\n",
    "train_copy = train_data.copy()\n",
    "test_copy = test_data.copy()\n",
    "\n",
    "train_copy['source'] = 'train'\n",
    "test_copy['source'] = 'test'\n",
    "test_copy['Item_Outlet_Sales'] = 0\n",
    "\n",
    "combined_data = pd.concat([train_copy, test_copy], ignore_index=True)\n",
    "print(f\"Combined data shape: {combined_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673607fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Item_Weight missing values...\n",
      "Missing Item_Weight: 2439\n",
      "Remaining missing Item_Weight: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values - Item_Weight\n",
    "print(\"Handling Item_Weight missing values...\")\n",
    "print(f\"Missing Item_Weight: {combined_data['Item_Weight'].isnull().sum()}\")\n",
    "\n",
    "# Use item identifier to fill missing weights\n",
    "weight_by_item = combined_data.groupby('Item_Identifier')['Item_Weight'].mean()\n",
    "missing_weight_mask = combined_data['Item_Weight'].isna()\n",
    "\n",
    "for idx in combined_data[missing_weight_mask].index:\n",
    "    item_id = combined_data.loc[idx, 'Item_Identifier']\n",
    "    if item_id in weight_by_item and not pd.isna(weight_by_item[item_id]):\n",
    "        combined_data.loc[idx, 'Item_Weight'] = weight_by_item[item_id]\n",
    "\n",
    "# Use item type for remaining missing values\n",
    "weight_by_type = combined_data.groupby('Item_Type')['Item_Weight'].mean()\n",
    "still_missing = combined_data['Item_Weight'].isna()\n",
    "for idx in combined_data[still_missing].index:\n",
    "    item_type = combined_data.loc[idx, 'Item_Type']\n",
    "    combined_data.loc[idx, 'Item_Weight'] = weight_by_type[item_type]\n",
    "\n",
    "# Fill any remaining with overall mean\n",
    "combined_data['Item_Weight'].fillna(combined_data['Item_Weight'].mean(), inplace=True)\n",
    "\n",
    "print(f\"Remaining missing Item_Weight: {combined_data['Item_Weight'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51f634a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handling Outlet_Size missing values...\n",
      "Missing Outlet_Size: 4016\n",
      "Remaining missing Outlet_Size: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values - Outlet_Size\n",
    "print(\"Handling Outlet_Size missing values...\")\n",
    "print(f\"Missing Outlet_Size: {combined_data['Outlet_Size'].isnull().sum()}\")\n",
    "\n",
    "# Fill based on Outlet_Type and Location patterns\n",
    "for outlet_type in combined_data['Outlet_Type'].unique():\n",
    "    for location in combined_data['Outlet_Location_Type'].unique():\n",
    "        mask = ((combined_data['Outlet_Type'] == outlet_type) & \n",
    "               (combined_data['Outlet_Location_Type'] == location))\n",
    "        subset = combined_data[mask]\n",
    "        \n",
    "        if len(subset) > 0 and not subset['Outlet_Size'].mode().empty:\n",
    "            mode_size = subset['Outlet_Size'].mode()[0]\n",
    "            null_mask = mask & combined_data['Outlet_Size'].isna()\n",
    "            combined_data.loc[null_mask, 'Outlet_Size'] = mode_size\n",
    "\n",
    "# Fill remaining by outlet type\n",
    "for outlet_type in combined_data['Outlet_Type'].unique():\n",
    "    type_mask = combined_data['Outlet_Type'] == outlet_type\n",
    "    if not combined_data[type_mask]['Outlet_Size'].mode().empty:\n",
    "        mode_size = combined_data[type_mask]['Outlet_Size'].mode()[0]\n",
    "        null_mask = type_mask & combined_data['Outlet_Size'].isna()\n",
    "        combined_data.loc[null_mask, 'Outlet_Size'] = mode_size\n",
    "\n",
    "print(f\"Remaining missing Outlet_Size: {combined_data['Outlet_Size'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3696b409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing Item_Fat_Content...\n",
      "Original values: ['Low Fat' 'Regular' 'low fat' 'LF' 'reg']\n",
      "Standardized values: ['Low Fat' 'Regular']\n"
     ]
    }
   ],
   "source": [
    "# Standardize Item_Fat_Content\n",
    "print(\"Standardizing Item_Fat_Content...\")\n",
    "print(\"Original values:\", combined_data['Item_Fat_Content'].unique())\n",
    "\n",
    "fat_content_mapping = {\n",
    "    'low fat': 'Low Fat',\n",
    "    'LF': 'Low Fat', \n",
    "    'reg': 'Regular',\n",
    "    'LOW FAT': 'Low Fat',\n",
    "    'REGULAR': 'Regular'\n",
    "}\n",
    "\n",
    "combined_data['Item_Fat_Content'] = combined_data['Item_Fat_Content'].replace(fat_content_mapping)\n",
    "print(\"Standardized values:\", combined_data['Item_Fat_Content'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41881a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal features...\n",
      "Outlet age categories:\n",
      "Outlet_Age_Category\n",
      "Established    5573\n",
      "Mature         5542\n",
      "New            3089\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create temporal features\n",
    "print(\"Creating temporal features...\")\n",
    "\n",
    "combined_data['Outlet_Years_Operating'] = 2013 - combined_data['Outlet_Establishment_Year']\n",
    "\n",
    "# Categorize outlet age\n",
    "combined_data['Outlet_Age_Category'] = pd.cut(\n",
    "    combined_data['Outlet_Years_Operating'], \n",
    "    bins=[0, 8, 15, 30], \n",
    "    labels=['New', 'Established', 'Mature']\n",
    ")\n",
    "\n",
    "print(\"Outlet age categories:\")\n",
    "print(combined_data['Outlet_Age_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18769515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating item identifier features...\n",
      "Item category codes:\n",
      "Item_Category_Code\n",
      "FD    10201\n",
      "NC     2686\n",
      "DR     1317\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extract item identifier features\n",
    "print(\"Creating item identifier features...\")\n",
    "\n",
    "combined_data['Item_Category_Code'] = combined_data['Item_Identifier'].str[:2]\n",
    "combined_data['Item_Numeric_ID'] = pd.to_numeric(\n",
    "    combined_data['Item_Identifier'].str[2:], errors='coerce'\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "print(\"Item category codes:\")\n",
    "print(combined_data['Item_Category_Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27b79f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating economic features...\n",
      "High value items: 3549\n",
      "Price/Weight ratio range: 1.67 - 50.78\n"
     ]
    }
   ],
   "source": [
    "# Create economic features\n",
    "print(\"Creating economic features...\")\n",
    "\n",
    "combined_data['Price_per_Weight_Ratio'] = combined_data['Item_MRP'] / combined_data['Item_Weight']\n",
    "combined_data['High_Value_Item'] = (\n",
    "    combined_data['Item_MRP'] > combined_data['Item_MRP'].quantile(0.75)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"High value items: {combined_data['High_Value_Item'].sum()}\")\n",
    "print(f\"Price/Weight ratio range: {combined_data['Price_per_Weight_Ratio'].min():.2f} - {combined_data['Price_per_Weight_Ratio'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "008fcea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item visibility...\n",
      "Items with zero visibility: 879\n",
      "Corrected visibility range: 0.003575 - 0.328391\n"
     ]
    }
   ],
   "source": [
    "# Handle item visibility\n",
    "print(\"Processing item visibility...\")\n",
    "\n",
    "combined_data['Has_Zero_Visibility'] = (combined_data['Item_Visibility'] == 0).astype(int)\n",
    "print(f\"Items with zero visibility: {combined_data['Has_Zero_Visibility'].sum()}\")\n",
    "\n",
    "# Correct zero visibility using item type averages\n",
    "visibility_by_type = combined_data[combined_data['Item_Visibility'] > 0].groupby('Item_Type')['Item_Visibility'].mean()\n",
    "combined_data['Item_Visibility_Corrected'] = combined_data['Item_Visibility'].copy()\n",
    "\n",
    "zero_visibility_mask = combined_data['Item_Visibility'] == 0\n",
    "for idx in combined_data[zero_visibility_mask].index:\n",
    "    item_type = combined_data.loc[idx, 'Item_Type']\n",
    "    combined_data.loc[idx, 'Item_Visibility_Corrected'] = visibility_by_type[item_type]\n",
    "\n",
    "print(f\"Corrected visibility range: {combined_data['Item_Visibility_Corrected'].min():.6f} - {combined_data['Item_Visibility_Corrected'].max():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c2b3e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating categorical bins...\n",
      "MRP segments:\n",
      "MRP_Price_Segment\n",
      "Economy     2842\n",
      "Budget      2841\n",
      "Premium     2841\n",
      "Standard    2840\n",
      "Luxury      2840\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Weight categories:\n",
      "Weight_Category\n",
      "Medium       3636\n",
      "Light        3575\n",
      "VeryHeavy    3508\n",
      "Heavy        3485\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create categorical bins\n",
    "print(\"Creating categorical bins...\")\n",
    "\n",
    "# MRP price segments\n",
    "combined_data['MRP_Price_Segment'] = pd.qcut(\n",
    "    combined_data['Item_MRP'], q=5, \n",
    "    labels=['Budget', 'Economy', 'Standard', 'Premium', 'Luxury']\n",
    ")\n",
    "\n",
    "# Weight categories\n",
    "combined_data['Weight_Category'] = pd.qcut(\n",
    "    combined_data['Item_Weight'], q=4, \n",
    "    labels=['Light', 'Medium', 'Heavy', 'VeryHeavy']\n",
    ")\n",
    "\n",
    "print(\"MRP segments:\")\n",
    "print(combined_data['MRP_Price_Segment'].value_counts())\n",
    "print(\"\\nWeight categories:\")\n",
    "print(combined_data['Weight_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae323059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating store performance features...\n",
      "Store performance features created\n"
     ]
    }
   ],
   "source": [
    "# Create store performance features\n",
    "print(\"Creating store performance features...\")\n",
    "\n",
    "train_mask = combined_data['source'] == 'train'\n",
    "if train_mask.sum() > 0:\n",
    "    store_performance = combined_data[train_mask].groupby('Outlet_Identifier')['Item_Outlet_Sales'].agg([\n",
    "        'mean', 'std', 'count', 'median', 'min', 'max'\n",
    "    ])\n",
    "    store_performance.columns = [\n",
    "        'Store_Avg_Sales', 'Store_Sales_Std', 'Store_Product_Count', \n",
    "        'Store_Median_Sales', 'Store_Min_Sales', 'Store_Max_Sales'\n",
    "    ]\n",
    "    store_performance['Store_Sales_Range'] = (\n",
    "        store_performance['Store_Max_Sales'] - store_performance['Store_Min_Sales']\n",
    "    )\n",
    "    \n",
    "    combined_data = combined_data.merge(\n",
    "        store_performance, left_on='Outlet_Identifier', right_index=True, how='left'\n",
    "    )\n",
    "    \n",
    "    # Fill missing values for test data\n",
    "    overall_stats = {\n",
    "        'Store_Avg_Sales': combined_data[train_mask]['Item_Outlet_Sales'].mean(),\n",
    "        'Store_Sales_Std': combined_data[train_mask]['Item_Outlet_Sales'].std(),\n",
    "        'Store_Product_Count': combined_data[train_mask].groupby('Outlet_Identifier').size().mean(),\n",
    "        'Store_Median_Sales': combined_data[train_mask]['Item_Outlet_Sales'].median(),\n",
    "        'Store_Min_Sales': combined_data[train_mask]['Item_Outlet_Sales'].min(),\n",
    "        'Store_Max_Sales': combined_data[train_mask]['Item_Outlet_Sales'].max(),\n",
    "        'Store_Sales_Range': combined_data[train_mask]['Item_Outlet_Sales'].max() - combined_data[train_mask]['Item_Outlet_Sales'].min()\n",
    "    }\n",
    "    \n",
    "    for col, value in overall_stats.items():\n",
    "        combined_data[col].fillna(value, inplace=True)\n",
    "\n",
    "print(\"Store performance features created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f58d2304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating market positioning features...\n",
      "Market segments:\n",
      "Market_Segment\n",
      "Standard    9539\n",
      "Budget      2841\n",
      "Premium     1824\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create market positioning features\n",
    "print(\"Creating market positioning features...\")\n",
    "\n",
    "combined_data['Market_Segment'] = 'Standard'\n",
    "\n",
    "# Premium segment\n",
    "premium_mask = (\n",
    "    (combined_data['Item_MRP'] > combined_data['Item_MRP'].quantile(0.8)) & \n",
    "    (combined_data['Item_Fat_Content'] == 'Low Fat')\n",
    ")\n",
    "combined_data.loc[premium_mask, 'Market_Segment'] = 'Premium'\n",
    "\n",
    "# Budget segment\n",
    "budget_mask = combined_data['Item_MRP'] < combined_data['Item_MRP'].quantile(0.2)\n",
    "combined_data.loc[budget_mask, 'Market_Segment'] = 'Budget'\n",
    "\n",
    "print(\"Market segments:\")\n",
    "print(combined_data['Market_Segment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ebabb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction features...\n",
      "Outlet-Item interactions: 64\n",
      "Size-Location interactions: 6\n"
     ]
    }
   ],
   "source": [
    "# Create interaction features\n",
    "print(\"Creating interaction features...\")\n",
    "\n",
    "combined_data['Outlet_Item_Interaction'] = (\n",
    "    combined_data['Outlet_Type'] + '_' + combined_data['Item_Type']\n",
    ")\n",
    "combined_data['Size_Location_Interaction'] = (\n",
    "    combined_data['Outlet_Size'].fillna('Unknown') + '_' + \n",
    "    combined_data['Outlet_Location_Type']\n",
    ")\n",
    "\n",
    "print(f\"Outlet-Item interactions: {combined_data['Outlet_Item_Interaction'].nunique()}\")\n",
    "print(f\"Size-Location interactions: {combined_data['Size_Location_Interaction'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63fc78ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating advanced ratio features...\n",
      "Visibility ratio range: 0.05 - 5.16\n",
      "MRP ratio range: 0.22 - 2.08\n"
     ]
    }
   ],
   "source": [
    "# Create advanced ratio features\n",
    "print(\"Creating advanced ratio features...\")\n",
    "\n",
    "# Visibility ratio compared to item type average\n",
    "item_type_avg_visibility = combined_data.groupby('Item_Type')['Item_Visibility_Corrected'].mean()\n",
    "combined_data['Visibility_Type_Ratio'] = combined_data.apply(\n",
    "    lambda x: x['Item_Visibility_Corrected'] / item_type_avg_visibility[x['Item_Type']], axis=1\n",
    ")\n",
    "\n",
    "# MRP ratio compared to item type average\n",
    "item_type_avg_mrp = combined_data.groupby('Item_Type')['Item_MRP'].mean()\n",
    "combined_data['MRP_Type_Ratio'] = combined_data.apply(\n",
    "    lambda x: x['Item_MRP'] / item_type_avg_mrp[x['Item_Type']], axis=1\n",
    ")\n",
    "\n",
    "print(f\"Visibility ratio range: {combined_data['Visibility_Type_Ratio'].min():.2f} - {combined_data['Visibility_Type_Ratio'].max():.2f}\")\n",
    "print(f\"MRP ratio range: {combined_data['MRP_Type_Ratio'].min():.2f} - {combined_data['MRP_Type_Ratio'].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be358411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding categorical features...\n",
      "Encoded Item_Fat_Content: 2 unique values\n",
      "Encoded Item_Type: 16 unique values\n",
      "Encoded Outlet_Size: 3 unique values\n",
      "Encoded Outlet_Location_Type: 3 unique values\n",
      "Encoded Outlet_Type: 4 unique values\n",
      "Encoded Item_Category_Code: 3 unique values\n",
      "Encoded Outlet_Age_Category: 3 unique values\n",
      "Encoded MRP_Price_Segment: 5 unique values\n",
      "Encoded Weight_Category: 4 unique values\n",
      "Encoded Market_Segment: 3 unique values\n",
      "Encoded Outlet_Item_Interaction: 64 unique values\n",
      "Encoded Size_Location_Interaction: 6 unique values\n",
      "\n",
      "Total features encoded: 12\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical features\n",
    "print(\"Encoding categorical features...\")\n",
    "\n",
    "categorical_features = [\n",
    "    'Item_Fat_Content', 'Item_Type', 'Outlet_Size', 'Outlet_Location_Type', \n",
    "    'Outlet_Type', 'Item_Category_Code', 'Outlet_Age_Category', 'MRP_Price_Segment', \n",
    "    'Weight_Category', 'Market_Segment', 'Outlet_Item_Interaction', 'Size_Location_Interaction'\n",
    "]\n",
    "\n",
    "label_encoders = {}\n",
    "for feature in categorical_features:\n",
    "    if feature in combined_data.columns:\n",
    "        combined_data[feature] = combined_data[feature].astype(str).replace('nan', 'Unknown')\n",
    "        le = LabelEncoder()\n",
    "        combined_data[feature] = le.fit_transform(combined_data[feature])\n",
    "        label_encoders[feature] = le\n",
    "        print(f\"Encoded {feature}: {len(le.classes_)} unique values\")\n",
    "\n",
    "print(f\"\\nTotal features encoded: {len(label_encoders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22213c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Summary:\n",
      "========================================\n",
      "Original features: 11\n",
      "Final features: 32\n",
      "Features added: 21\n",
      "\n",
      "New features created:\n",
      "- Outlet_Years_Operating\n",
      "- Outlet_Age_Category\n",
      "- Item_Category_Code\n",
      "- Item_Numeric_ID\n",
      "- Price_per_Weight_Ratio\n",
      "- High_Value_Item\n",
      "- Has_Zero_Visibility\n",
      "- Item_Visibility_Corrected\n",
      "- MRP_Price_Segment\n",
      "- Weight_Category\n",
      "- Store_Avg_Sales\n",
      "- Store_Sales_Std\n",
      "- Store_Product_Count\n",
      "- Store_Median_Sales\n",
      "- Store_Min_Sales\n",
      "- Store_Max_Sales\n",
      "- Store_Sales_Range\n",
      "- Market_Segment\n",
      "- Outlet_Item_Interaction\n",
      "- Size_Location_Interaction\n",
      "- Visibility_Type_Ratio\n",
      "- MRP_Type_Ratio\n",
      "\n",
      "Final dataset shape: (14204, 35)\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Feature summary\n",
    "print(\"Feature Engineering Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Original features: {len(train_data.columns)-1}\")\n",
    "print(f\"Final features: {len(combined_data.columns)-3}\")\n",
    "print(f\"Features added: {len(combined_data.columns)-3-(len(train_data.columns)-1)}\")\n",
    "\n",
    "print(\"\\nNew features created:\")\n",
    "new_features = [\n",
    "    'Outlet_Years_Operating', 'Outlet_Age_Category', 'Item_Category_Code', 'Item_Numeric_ID',\n",
    "    'Price_per_Weight_Ratio', 'High_Value_Item', 'Has_Zero_Visibility', 'Item_Visibility_Corrected',\n",
    "    'MRP_Price_Segment', 'Weight_Category', 'Store_Avg_Sales', 'Store_Sales_Std', 'Store_Product_Count',\n",
    "    'Store_Median_Sales', 'Store_Min_Sales', 'Store_Max_Sales', 'Store_Sales_Range',\n",
    "    'Market_Segment', 'Outlet_Item_Interaction', 'Size_Location_Interaction',\n",
    "    'Visibility_Type_Ratio', 'MRP_Type_Ratio'\n",
    "]\n",
    "\n",
    "for feature in new_features:\n",
    "    if feature in combined_data.columns:\n",
    "        print(f\"- {feature}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {combined_data.shape}\")\n",
    "print(f\"Missing values: {combined_data.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48fc4301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineered training data: (8523, 34)\n",
      "Engineered test data: (5681, 33)\n",
      "\n",
      "Saving engineered datasets...\n",
      "Feature engineering completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Split back to train and test\n",
    "train_engineered = combined_data[combined_data['source'] == 'train'].copy()\n",
    "test_engineered = combined_data[combined_data['source'] == 'test'].copy()\n",
    "\n",
    "# Remove helper columns\n",
    "train_engineered = train_engineered.drop('source', axis=1)\n",
    "test_engineered = test_engineered.drop(['source', 'Item_Outlet_Sales'], axis=1)\n",
    "\n",
    "print(f\"Engineered training data: {train_engineered.shape}\")\n",
    "print(f\"Engineered test data: {test_engineered.shape}\")\n",
    "\n",
    "# Save engineered datasets\n",
    "print(\"\\nSaving engineered datasets...\")\n",
    "train_engineered.to_csv('outputs/train_engineered.csv', index=False)\n",
    "test_engineered.to_csv('outputs/test_engineered.csv', index=False)\n",
    "print(\"Feature engineering completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "V3_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
